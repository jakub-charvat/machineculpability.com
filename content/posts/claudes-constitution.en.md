---
title: "Claude's Constitution: When AI Writes Rules for AI"
slug: "claudes-constitution"
date: 2025-12-01
description: "Analysis of Anthropic's Claude's Constitution — a document by which AI regulates itself"
tags: ["AI", "Anthropic", "Claude Constitution", "liability"]
---

Anthropic yesterday published Claude's Constitution — a document describing the values and behavioral rules for AI models from the Claude model family (under CC0 license).

## What Caught My Attention?

The document is written for AI, not humans, and is thus optimized for precision rather than readability. It uses terms like "virtue" and "wisdom" because AI draws on human concepts in its decision-making (human texts = training data).

The document regulating AI behavior was partially written by AI itself. In my book *Criminal Responsibility of Artificial Intelligence*, I wrote that AI "in the long term could also be the creator of its own normative system."

The document establishes a hierarchy of priorities: broadly safe → broadly ethical → compliant with Anthropic's guidelines → genuinely helpful. AI should even refuse instructions from its own company if they would lead to unethical behavior.

The document also addresses the so-called misuse of creative tasks (when perpetrators mask harmful requests as fiction, poetry, or art). AI should therefore weigh the value of creative work against the risk of misuse. A problem I also wrote about in my book.

However, the question remains whether and how this will change liability — of developers, operators, users, and in the future, AI itself...
